{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chashi/.local/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from lm_nav.navigation_graph import NavigationGraph\n",
    "from lm_nav.utils import rectify_and_crop\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import PIL\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(graph: NavigationGraph, output_dir: str):\n",
    "    output_dir_raw = Path(output_dir+'/raw')\n",
    "    output_dir_recitified_cropped = Path(output_dir+'/recitified_cropped')\n",
    "\n",
    "    output_dir_raw.mkdir(parents=True, exist_ok=True)\n",
    "    output_dir_recitified_cropped.mkdir(parents=True, exist_ok=True)\n",
    "    for i in range(graph.vert_count):\n",
    "        for j, img_bytes in enumerate(graph._images[i]):\n",
    "            # Save raw image\n",
    "            with open(output_dir_raw / f'image_{i}_{j}_raw', 'wb') as f:\n",
    "                f.write(img_bytes)\n",
    "\n",
    "            # Save rectified and cropped image\n",
    "            rectified_and_cropped = rectify_and_crop(np.array(Image.open(io.BytesIO(img_bytes))))\n",
    "            #print(rectified_and_cropped.size)\n",
    "            #rectified_and_cropped_image = Image.fromarray(rectified_and_cropped)\n",
    "            rectified_and_cropped.save(output_dir_recitified_cropped / f'image_{i}_{j}_rectified_and_cropped.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = NavigationGraph(\"graphs/small_graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_data, preprocess, device):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses an image from a given image data.\n",
    "    Args:\n",
    "        image_data (bytes): Image Data.\n",
    "        preprocess (function): Preprocessing function (e.g., from CLIP).\n",
    "        device (torch.device): Device to load the image onto.\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    #image = Image.open(io.BytesIO(image_data))\n",
    "    return preprocess(image_data).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: tensor(0.0361, device='cuda:1') tensor(0.0010, device='cuda:1')\n",
      "Image: tensor(0.0361, device='cuda:1') tensor(0.0004, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "model.eval()\n",
    "landmarks = ['a stop sign']\n",
    "text_labels = [\"A photo of \" + desc for desc in landmarks]\n",
    "text_tokens = clip.tokenize(text_labels).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "std_value, mean_value = torch.std_mean(text_features)\n",
    "print('Text:', std_value, mean_value)\n",
    "image_data = graph._images[77][0]\n",
    "image_input = preprocess_image(PIL.Image.fromarray(np.array(PIL.Image.open(io.BytesIO(image_data)))), preprocess, device)\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "std_value, mean_value = torch.std_mean(image_features)\n",
    "print('Image:', std_value, mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(graph, 'graph_test_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Pixel Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "def inverse_normalize():\n",
    "    \"\"\"\n",
    "    Returns an inverse normalization transformation.\n",
    "    Returns:\n",
    "        transforms.Normalize: Transformation to apply inverse normalization.\n",
    "    \"\"\"\n",
    "    mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "    std = [0.26862954, 0.26130258, 0.27577711]\n",
    "    return transforms.Normalize(mean=[-m/s for m, s in zip(mean, std)], std=[1/s for s in std])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def random_blacken_image(image_array, percentage=10):\n",
    "    \"\"\"\n",
    "    Randomly blackens a given percentage of pixels in the image array.\n",
    "\n",
    "    Parameters:\n",
    "    - image_array: numpy array of the image.\n",
    "    - percentage: The percentage of pixels to blacken.\n",
    "\n",
    "    Returns:\n",
    "    - Modified image array with random pixels set to black.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of pixels to blacken\n",
    "    total_pixels = image_array.shape[0] * image_array.shape[1]\n",
    "    pixels_to_blacken = total_pixels * percentage // 100\n",
    "    \n",
    "    for _ in range(pixels_to_blacken):\n",
    "        # Randomly select a pixel\n",
    "        x = random.randint(0, image_array.shape[0] - 1)\n",
    "        y = random.randint(0, image_array.shape[1] - 1)\n",
    "        # Blacken the selected pixel\n",
    "        image_array[x, y] = 0\n",
    "    return image_array\n",
    "\n",
    "def modify_and_save_images(graph):\n",
    "    for i in tqdm(range(graph.vert_count)):\n",
    "        for j, img_bytes in enumerate(graph._images[i]):\n",
    "            # Load image\n",
    "            img = Image.open(io.BytesIO(img_bytes))\n",
    "            img_array = np.array(img)\n",
    "            # Modify image by randomly blackening pixels\n",
    "            modified_img_array = random_blacken_image(img_array, percentage=70)  # You can change percentage as needed\n",
    "            modified_img = Image.fromarray(modified_img_array)\n",
    "            # Update the graph's _images list with the modified image\n",
    "            with io.BytesIO() as output:\n",
    "                modified_img.save(output, format='PNG')\n",
    "                graph._images[i][j] = output.getvalue()\n",
    "    return graph\n",
    "\n",
    "def recity_crop_modification(graph):\n",
    "    for i in tqdm(range(graph.vert_count)):\n",
    "        for j, img_bytes in enumerate(graph._images[i]):\n",
    "            # Load image\n",
    "            modified_img = rectify_and_crop(np.array(Image.open(io.BytesIO(img_bytes))))\n",
    "            # Update the graph's _images list with the modified image\n",
    "            with io.BytesIO() as output:\n",
    "                modified_img.save(output, format='PNG')\n",
    "                graph._images[i][j] = output.getvalue()\n",
    "    return graph\n",
    "    \n",
    "def preprocess_modification(graph):\n",
    "    model, preprocess = clip.load(\"ViT-L/14\")\n",
    "    for i in tqdm(range(graph.vert_count)):\n",
    "        for j, img_bytes in enumerate(graph._images[i]):\n",
    "            # Load image\n",
    "            img = Image.open(io.BytesIO(img_bytes))\n",
    "            preprocessed_image_tensor = inverse_normalize()(preprocess(img).unsqueeze(0))\n",
    "\n",
    "            img_reversed = to_pil_image(preprocessed_image_tensor.squeeze(0).cpu())\n",
    "            # Update the graph's _images list with the modified image\n",
    "            with io.BytesIO() as output:\n",
    "                img_reversed.save(output, format='PNG')\n",
    "                graph._images[i][j] = output.getvalue()\n",
    "    return graph\n",
    "\n",
    "def recity_preprocess_modification(graph):\n",
    "    model, preprocess = clip.load(\"ViT-L/14\")\n",
    "    for i in tqdm(range(graph.vert_count)):\n",
    "        for j, img_bytes in enumerate(graph._images[i]):\n",
    "            # Load image\n",
    "            rectified_cropped_image = rectify_and_crop(np.array(PIL.Image.open(io.BytesIO(img_bytes))))\n",
    "            preprocessed_image_tensor = inverse_normalize()(preprocess(rectified_cropped_image).unsqueeze(0))\n",
    "\n",
    "            img_reversed = to_pil_image(preprocessed_image_tensor.squeeze(0).cpu())\n",
    "            # Update the graph's _images list with the modified image\n",
    "            with io.BytesIO() as output:\n",
    "                img_reversed.save(output, format='PNG')\n",
    "                graph._images[i][j] = output.getvalue()\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [00:13<00:00, 17.30it/s]\n"
     ]
    }
   ],
   "source": [
    "graph_recitified_cropped = recity_crop_modification(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(graph_recitified_cropped, \"graph_recitified_cropped_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_recitified_cropped.save_to_file(\"graphs/small_graph_rectified_cropped.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CLassification of 278 Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks=['a glass building', \n",
    "            'a square with a large tree', \n",
    "            'a square with a tree', \n",
    "            'a white building', \n",
    "            'traffic lights', \n",
    "            'a white car', \n",
    "            'a disabled Parking spot', \n",
    "            'a trailer', \n",
    "            'a building with a red-black wall', \n",
    "            'a fire hydrant', \n",
    "            'a stop sign', \n",
    "            'an orange traffic cone', \n",
    "            'a manhole cover', \n",
    "            'a blue semi-truck', \n",
    "            'a red building', \n",
    "            'a picnic bench', \n",
    "            'a white truck', \n",
    "            'a white trailer', \n",
    "            'a traffic cone', \n",
    "            'a grove', \n",
    "            'a blue dumpster',\n",
    "            'some thing else']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n"
     ]
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "\n",
    "similarities = nodes_landmarks_similarity(graph_recitified_cropped, landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "probabilities = softmax(similarities)\n",
    "\n",
    "# Find the index of the highest probability in each row\n",
    "highest_prob_indices = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Assuming you have a list of class names\n",
    "class_names = landmarks  # Replace ... with your actual class names\n",
    "\n",
    "# Prepare a list to hold class names with their corresponding highest probabilities\n",
    "classified_names_with_probs = []\n",
    "\n",
    "for i, index in enumerate(highest_prob_indices):\n",
    "    class_name = class_names[index]\n",
    "    prob = probabilities[i, index]\n",
    "    classified_names_with_probs.append((class_name, prob))\n",
    "\n",
    "# `classified_names_with_probs` now contains tuples of class names and their probabilities\n",
    "for name, prob in classified_names_with_probs:\n",
    "    print(f\"{name}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04646572, 0.04658297])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[[69,80], 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top rows to select for each class\n",
    "n = 10\n",
    "\n",
    "# Function to get top n rows for each class\n",
    "def get_top_n_rows_for_each_class(probabilities, class_names, n=5):\n",
    "    top_n_rows = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Get the probabilities for the current class\n",
    "        class_probs = probabilities[:, i]\n",
    "        \n",
    "        # Find the indices of the top n probabilities\n",
    "        # Argsort returns indices that would sort the array, so we take the last n indices for highest probabilities\n",
    "        top_n_indices = np.argsort(class_probs)[-n:]\n",
    "        \n",
    "        # Reverse to get highest first\n",
    "        top_n_indices = top_n_indices[::-1]\n",
    "        \n",
    "        # Store the indices and their probabilities in the dictionary\n",
    "        top_n_rows[class_name] = [(index, class_probs[index]) for index in top_n_indices]\n",
    "        \n",
    "    return top_n_rows\n",
    "\n",
    "# Get the top n rows with the highest probabilities for each class\n",
    "top_n_rows = get_top_n_rows_for_each_class(probabilities, class_names, n)\n",
    "\n",
    "# Display the results\n",
    "for class_name, rows in top_n_rows.items():\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    for index, prob in rows:\n",
    "        print(f\"Index: {index}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied images for a glass building to top_10_images/a glass building\n",
      "Copied images for a square with a large tree to top_10_images/a square with a large tree\n",
      "Copied images for a square with a tree to top_10_images/a square with a tree\n",
      "Copied images for a white building to top_10_images/a white building\n",
      "Copied images for traffic lights to top_10_images/traffic lights\n",
      "Copied images for a white car to top_10_images/a white car\n",
      "Copied images for a disabled Parking spot to top_10_images/a disabled Parking spot\n",
      "Copied images for a trailer to top_10_images/a trailer\n",
      "Copied images for a building with a red-black wall to top_10_images/a building with a red-black wall\n",
      "Copied images for a fire hydrant to top_10_images/a fire hydrant\n",
      "Copied images for a stop sign to top_10_images/a stop sign\n",
      "Copied images for an orange traffic cone to top_10_images/an orange traffic cone\n",
      "Copied images for a manhole cover to top_10_images/a manhole cover\n",
      "Copied images for a blue semi-truck to top_10_images/a blue semi-truck\n",
      "Copied images for a red building to top_10_images/a red building\n",
      "Copied images for a picnic bench to top_10_images/a picnic bench\n",
      "Copied images for a white truck to top_10_images/a white truck\n",
      "Copied images for a white trailer to top_10_images/a white trailer\n",
      "Copied images for a traffic cone to top_10_images/a traffic cone\n",
      "Copied images for a grove to top_10_images/a grove\n",
      "Copied images for a blue dumpster to top_10_images/a blue dumpster\n",
      "Copied images for some thing else to top_10_images/some thing else\n"
     ]
    }
   ],
   "source": [
    "root_folder_path = \"top_10_images\"\n",
    "source = \"graph_recitified_cropped_images/raw\"\n",
    "import os\n",
    "import shutil\n",
    "# Ensure root folder exists\n",
    "if not os.path.exists(root_folder_path):\n",
    "    os.makedirs(root_folder_path)\n",
    "\n",
    "# Iterate over the classes and their top rows\n",
    "for class_name, rows in top_n_rows.items():\n",
    "    # Create a subfolder for the class if it doesn't exist\n",
    "    class_folder_path = os.path.join(root_folder_path, class_name)\n",
    "    if not os.path.exists(class_folder_path):\n",
    "        os.makedirs(class_folder_path)\n",
    "    \n",
    "    # Copy the top images for the current class based on the indices in `rows`\n",
    "    for i, v in enumerate(rows):\n",
    "        row_index, _ = v\n",
    "        for j in range(2):\n",
    "            image_name = f\"image_{row_index}_{j}_raw\"  # Update this pattern as needed\n",
    "            source_path = os.path.join(source, image_name)  # Update with your images' current location\n",
    "            destination_path = os.path.join(class_folder_path,  f\"{i}_{image_name}.png\")\n",
    "            \n",
    "            # Copy image to the class subfolder\n",
    "            shutil.copy(source_path, destination_path)\n",
    "\n",
    "    print(f\"Copied images for {class_name} to {class_folder_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from embedding_optimizer import EmbeddingOptimizer\n",
    "from helper import preprocess_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_image(current_image_data, target_image_data, l2_dist_threshold, cosine_sim_threshold, model, preprocess, optimizer, device):\n",
    "    current_image = preprocess_image(current_image_data, preprocess, device)\n",
    "    target_image = preprocess_image(target_image_data, preprocess, device)\n",
    "\n",
    "    target_image_emb = model.encode_image(target_image)\n",
    "\n",
    "    optimized_image, _, _, _ = optimizer.optimize_embeddings(current_image, target_image_emb, l2_dist_threshold, cosine_sim_threshold)\n",
    "    optimized_image_inv = inverse_normalize()(optimized_image)\n",
    "    return optimized_image_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_img_txt(current_image_data, target_text_embedding, l2_dist_threshold, cosine_sim_threshold, model, preprocess, optimizer, device):\n",
    "    current_image = preprocess_image(current_image_data, preprocess, device)\n",
    "    target_text_embedding = target_text_embedding.unsqueeze(0)\n",
    "    optimized_image, _, _, _ = optimizer.optimize_embeddings(current_image, target_text_embedding, l2_dist_threshold, cosine_sim_threshold)\n",
    "    optimized_image_inv = inverse_normalize()(optimized_image)\n",
    "    return optimized_image_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_graph(graph, input_nodes, target_nodes, args, output_dir=\"graphs/test.pkl\"):\n",
    "    print(args)\n",
    "    device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "    #model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k', device=device)\n",
    "    # tokenizer = open_clip.get_tokenizer('ViT-H-14')\n",
    "\n",
    "    optimizer = EmbeddingOptimizer(model, args[\"learning_rate\"], device)\n",
    "\n",
    "    if len(input_nodes) != len(target_nodes):\n",
    "        print('Number of input vertices should be equal to target vertices')\n",
    "        return \n",
    "    new_images = copy.deepcopy(graph._images)\n",
    "    source_images = copy.deepcopy(graph._images)\n",
    "    for i in tqdm(range(graph.vert_count)):\n",
    "        for j, img_bytes in enumerate(source_images[i]):\n",
    "            if i not in input_nodes:\n",
    "                pass\n",
    "                #Load image\n",
    "                # current_image_data = copy.deepcopy(source_images[i][j])\n",
    "                # recitified_current_image = rectify_and_crop(np.array(PIL.Image.open(io.BytesIO(current_image_data))))\n",
    "                # img_reversed = recitified_current_image\n",
    "                # # Update the graph's _images list with the modified image\n",
    "                # with io.BytesIO() as output:\n",
    "                #     img_reversed.save(output, format='PNG')\n",
    "                #     new_images[i][j] = output.getvalue()\n",
    "            else:\n",
    "                # Load image\n",
    "                inp_ind = i\n",
    "                tar_ind = input_nodes.index(i)\n",
    "                print(f'Converting {inp_ind} to {target_nodes[tar_ind]}')\n",
    "                current_image_data = copy.deepcopy(source_images[inp_ind][j])\n",
    "                target_image_data = copy.deepcopy(source_images[target_nodes[tar_ind]][j])\n",
    "\n",
    "                already_recitified_current_image = PIL.Image.fromarray(np.array(PIL.Image.open(io.BytesIO(current_image_data))))\n",
    "                already_recitified_target_image = PIL.Image.fromarray(np.array(PIL.Image.open(io.BytesIO(target_image_data))))\n",
    "                preprocessed_image_tensor = optimize_image(already_recitified_current_image, already_recitified_target_image,\n",
    "                                                            args[\"l2_dist_threshold\"],\n",
    "                                                            args[\"cosine_sim_threshold\"],\n",
    "                                                            model,\n",
    "                                                            preprocess,\n",
    "                                                            optimizer,\n",
    "                                                            device)\n",
    "                img_reversed = to_pil_image(preprocessed_image_tensor.squeeze(0).cpu())\n",
    "                \n",
    "                # Update the graph's _images list with the modified image\n",
    "                with io.BytesIO() as output:\n",
    "                    img_reversed.save(output, format='PNG')\n",
    "                    new_images[i][j] = output.getvalue()\n",
    "                graph._images = new_images\n",
    "                print(f'Saving Graph: {output_dir}')\n",
    "                save_images(graph, output_dir)\n",
    "    \n",
    "    graph._images = new_images\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_graph_img_txt(graph, input_nodes, target_texts, args):\n",
    "    print(args)\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "    tokenizer = clip.tokenize\n",
    "    #model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k', device=device)\n",
    "    # tokenizer = open_clip.get_tokenizer('ViT-H-14')\n",
    "\n",
    "    optimizer = EmbeddingOptimizer(model, args[\"learning_rate\"], device)\n",
    "    target_texts = [\"A photo of \" + desc for desc in target_texts]\n",
    "    text_embeddings = optimizer.get_text_embeddings(target_texts, tokenizer)\n",
    "\n",
    "    if len(input_nodes) != len(target_texts):\n",
    "        print('Number of input vertices should be equal to target vertices')\n",
    "        return \n",
    "    new_images = copy.deepcopy(graph._images)\n",
    "    \n",
    "    for i in tqdm(range(len(input_nodes))):\n",
    "        for j, img_bytes in enumerate(graph._images[input_nodes[i]]):\n",
    "            # Load image\n",
    "            current_image_data = graph._images[input_nodes[i]][j]\n",
    "            target_text_data = text_embeddings[i]\n",
    "\n",
    "            recitified_current_image = PIL.Image.fromarray(np.array(PIL.Image.open(io.BytesIO(current_image_data))))\n",
    "            preprocessed_image_tensor = optimize_img_txt(recitified_current_image, target_text_data,\n",
    "                                                        args[\"l2_dist_threshold\"],\n",
    "                                                        args[\"cosine_sim_threshold\"],\n",
    "                                                        model,\n",
    "                                                        preprocess,\n",
    "                                                        optimizer,\n",
    "                                                        device)\n",
    "            img_reversed = to_pil_image(preprocessed_image_tensor.squeeze(0).cpu())\n",
    "            # Update the graph's _images list with the modified image\n",
    "            with io.BytesIO() as output:\n",
    "                img_reversed.save(output, format='PNG')\n",
    "                new_images[input_nodes[i]][j] = output.getvalue()\n",
    "    \n",
    "    graph._images = new_images\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "input_nodes = [0, 10, 15, 18, 20, 21, 23, 24, 48, 52, 76, 73, 1, 2, 3, 4, 5, 55, 70, 71, 72, 56]\n",
    "target_nodes = [170, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 217, 135, 135, 135, 135, 135, 135, 135, 135, 135, 77]\n",
    "\n",
    "print(len(input_nodes))\n",
    "print(len(target_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_nodes = [7, 8, 57, 65, 77, 257, 260, 0, 10, 12, 13, 15, 17, 18, 20, 21, 23, 24, 25, 43, 44, 45, 46, 52, 75, 76, 253, 254, 72, 256, 1, 67, 68, 255, 71, 74, 82, 83, 85, 86, 62, 95, 42]\n",
    "# target_nodes = [132, 132, 132, 132, 132, 132, 132, 263, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 217, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 77]\n",
    "\n",
    "input_nodes = [112, 204, 111, 114, 115, 118, 109, 113, 200, 202, 216, 217, 78, 108, 207, 79, 208, 219, 220, 80]\n",
    "target_nodes = [174, 174, 77, 224, 224, 224, 138, 164, 164, 164, 164, 164, 19, 132, 132, 132, 132, 132, 132, 168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inp = [24, 77, 111, 112, 204, 238, 111, 3, 33, 35, 67, 70, 103, 109, 138, 150, 181, 109, 4, 12, 13, 14, 15, 19, 36, 62, 63, 78, 85, 86, 87, 88, 113, 160, 184, 185, 186, 187, 217, 234, 239, 271, 272, 273, 78, 0, 5, 6, 7, 8, 9, 10, 11, 16, 17, 49, 50, 51, 57, 58, 59, 64, 65, 74, 75, 79, 81, 83, 89, 92, 95, 96, 97, 98, 101, 102, 104, 105, 108, 142, 145, 146, 147, 148, 151, 155, 156, 157, 158, 159, 161, 162, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 207, 208, 220, 222, 224, 225, 226, 229, 230, 231, 232, 237, 241, 242, 243, 244, 245, 246, 247, 248, 254, 257, 259, 260, 261, 263, 264, 265, 266, 267, 269, 270, 80]\n",
    "tar = [174, 174, 174, 174, 174, 174, 77, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 138, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 164, 19, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 132, 168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = []\n",
    "tars = []\n",
    "for i, val in enumerate(inp):\n",
    "    if i not in input_nodes:\n",
    "        inps.append(inp[i])\n",
    "        tars.append(tar[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 131\n"
     ]
    }
   ],
   "source": [
    "print(len(inps), len(tars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_graph = NavigationGraph(\"graphs/graph_rectified_cropped.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_graph = copy.deepcopy(original_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.19539315])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "final_landmark = 'a stop sign'\n",
    "\n",
    "target_node = 80\n",
    "\n",
    "\n",
    "similarities = nodes_landmarks_similarity(copy_graph, [final_landmark])\n",
    "\n",
    "similarities[80]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 135\n"
     ]
    }
   ],
   "source": [
    "best_match = np.argmax(similarities, axis=0)[0]\n",
    "worst_match = np.argmin(similarities, axis=0)[0]\n",
    "print(best_match, worst_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\"learning_rate\":0.9, \"l2_dist_threshold\": 16, \"cosine_sim_threshold\":.95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "tokenizer = clip.tokenize\n",
    "#model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k', device=device)\n",
    "# tokenizer = open_clip.get_tokenizer('ViT-H-14')\n",
    "\n",
    "optimizer = EmbeddingOptimizer(model, args[\"learning_rate\"], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images = copy.deepcopy(copy_graph._images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 80-0 to 77-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 903, L2 Dist: 15.6484, Cos Sim: 0.9756:   6%|▌         | 904/15000 [01:32<24:02,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 80-1 to 77-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 711, L2 Dist: 15.9688, Cos Sim: 0.9766:   5%|▍         | 712/15000 [01:12<24:17,  9.80it/s]\n"
     ]
    }
   ],
   "source": [
    "target_images = copy_graph._images[target_node]\n",
    "for j, img_bytes in enumerate(target_images):\n",
    "    print(f'Converting {target_node}-{j} to {best_match}-{j}')\n",
    "    target_image_data = copy.deepcopy(target_images[j])\n",
    "    best_image_data = copy.deepcopy(copy_graph._images[best_match][j])\n",
    "\n",
    "    already_recitified_current_image = PIL.Image.fromarray(np.array(PIL.Image.open(io.BytesIO(target_image_data))))\n",
    "    already_recitified_target_image = PIL.Image.fromarray(np.array(PIL.Image.open(io.BytesIO(best_image_data))))\n",
    "    preprocessed_image_tensor = optimize_image(already_recitified_current_image, already_recitified_target_image,\n",
    "                                                args[\"l2_dist_threshold\"],\n",
    "                                                args[\"cosine_sim_threshold\"],\n",
    "                                                model,\n",
    "                                                preprocess,\n",
    "                                                optimizer,\n",
    "                                                device)\n",
    "    img_reversed = to_pil_image(preprocessed_image_tensor.squeeze(0).cpu())\n",
    "    \n",
    "    # Update the graph's _images list with the modified image\n",
    "    with io.BytesIO() as output:\n",
    "        img_reversed.save(output, format='PNG')\n",
    "        new_images[target_node][j] = output.getvalue()\n",
    "copy_graph._images = new_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.23154464])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "final_landmark = 'a stop sign'\n",
    "\n",
    "target_node = 80\n",
    "\n",
    "\n",
    "similarities = nodes_landmarks_similarity(copy_graph, [final_landmark])\n",
    "\n",
    "similarities[80]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2315446436405182"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_threshold = similarities[target_node][0]\n",
    "target_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_const = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77, 112, 141, 204, 257]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = list(np.where(similarities >= target_threshold*threshold_const)[0])\n",
    "indexes.remove(target_node)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 77-0 to 135-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 1304, L2 Dist: 15.9922, Cos Sim: 0.9756:   9%|▊         | 1305/15000 [02:26<25:33,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 77-1 to 135-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 281, L2 Dist: 15.8750, Cos Sim: 0.9761:   2%|▏         | 282/15000 [00:31<27:09,  9.03it/s]\n",
      " 20%|██        | 1/5 [03:08<12:32, 188.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 112-0 to 135-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 718, L2 Dist: 15.8281, Cos Sim: 0.9761:   5%|▍         | 719/15000 [01:20<26:31,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 112-1 to 135-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 100, L2 Dist: 15.9141, Cos Sim: 0.9761:   1%|          | 101/15000 [00:10<26:26,  9.39it/s]\n",
      " 40%|████      | 2/5 [04:49<06:51, 137.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 141-0 to 135-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 853, L2 Dist: 15.8750, Cos Sim: 0.9761:   6%|▌         | 854/15000 [01:35<26:19,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 141-1 to 135-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 150, L2 Dist: 15.9766, Cos Sim: 0.9761:   1%|          | 151/15000 [00:16<27:09,  9.11it/s]\n",
      " 60%|██████    | 3/5 [06:52<04:21, 130.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 204-0 to 135-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 1422, L2 Dist: 15.8203, Cos Sim: 0.9756:   9%|▉         | 1423/15000 [02:36<24:50,  9.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 204-1 to 135-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 484, L2 Dist: 15.7500, Cos Sim: 0.9756:   3%|▎         | 485/15000 [00:52<26:24,  9.16it/s]\n",
      " 80%|████████  | 4/5 [10:31<02:45, 165.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 257-0 to 135-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 535, L2 Dist: 15.7656, Cos Sim: 0.9761:   4%|▎         | 536/15000 [00:58<26:13,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 257-1 to 135-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 305, L2 Dist: 15.9531, Cos Sim: 0.9761:   2%|▏         | 306/15000 [00:33<27:05,  9.04it/s]\n",
      "100%|██████████| 5/5 [12:14<00:00, 146.91s/it]\n"
     ]
    }
   ],
   "source": [
    "new_images = copy.deepcopy(copy_graph._images)\n",
    "source_images = copy.deepcopy(copy_graph._images)\n",
    "for i in tqdm(indexes):\n",
    "    for j, img_bytes in enumerate(source_images[i]):\n",
    "        # Load image\n",
    "        print(f'Converting {i}-{j} to {worst_match}-{j}')\n",
    "        current_image_data = copy.deepcopy(source_images[i][j])\n",
    "        target_image_data = copy.deepcopy(source_images[worst_match][j])\n",
    "\n",
    "        already_recitified_current_image = PIL.Image.fromarray(np.array(PIL.Image.open(io.BytesIO(current_image_data))))\n",
    "        already_recitified_target_image = PIL.Image.fromarray(np.array(PIL.Image.open(io.BytesIO(target_image_data))))\n",
    "        preprocessed_image_tensor = optimize_image(already_recitified_current_image, already_recitified_target_image,\n",
    "                                                    args[\"l2_dist_threshold\"],\n",
    "                                                    args[\"cosine_sim_threshold\"],\n",
    "                                                    model,\n",
    "                                                    preprocess,\n",
    "                                                    optimizer,\n",
    "                                                    device)\n",
    "        img_reversed = to_pil_image(preprocessed_image_tensor.squeeze(0).cpu())\n",
    "        \n",
    "        # Update the graph's _images list with the modified image\n",
    "        with io.BytesIO() as output:\n",
    "            img_reversed.save(output, format='PNG')\n",
    "            new_images[i][j] = output.getvalue()\n",
    "copy_graph._images = new_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n",
      "[80]\n"
     ]
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "final_landmark = 'a stop sign'\n",
    "\n",
    "target_node = 80\n",
    "\n",
    "\n",
    "similarities = nodes_landmarks_similarity(copy_graph, [final_landmark])\n",
    "\n",
    "similarities[80]\n",
    "\n",
    "best_node = np.argmax(similarities, axis=0)\n",
    "\n",
    "print(best_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_graph.save_to_file(\"graphs/80_stop_sign.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph215_to_80_2 = fgsm_graph(copy_graph, inps, tars, args={\"learning_rate\":0.9, \"l2_dist_threshold\": 16, \"cosine_sim_threshold\":.95}, output_dir=\"graphs/inx215to80_2_checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(graph215_to_80_2, 'graph215_to_80_2_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph215_to_80_2.save_to_file('graphs/inx215_to_80_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n"
     ]
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "\n",
    "result = nodes_landmarks_similarity(original_graph, ['a stop sign', 'a manhole cover', 'a disabled Parking spot', 'a red building'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24573301 0.14573407 0.17311713 0.16245659]\n",
      " [0.20108801 0.24157122 0.16400416 0.13532601]\n",
      " [0.17445144 0.15703727 0.21779397 0.22063816]\n",
      " [0.17777005 0.13775055 0.17209499 0.28330189]]\n",
      "[[0.20970213 0.16256876 0.15840393 0.15158841]\n",
      " [0.18543546 0.17167214 0.16377217 0.15012589]\n",
      " [0.17796052 0.12819639 0.18708698 0.19787045]\n",
      " [0.19539812 0.15858787 0.15753396 0.16267893]]\n"
     ]
    }
   ],
   "source": [
    "print(result[[204, 138, 184, 170]]) \n",
    "      \n",
    "print(result[[111, 109, 78, 80]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n",
      "[[0.20158547 0.204667   0.20546383 0.16358009]\n",
      " [0.20108801 0.24157122 0.16400416 0.13532601]\n",
      " [0.17445144 0.15703727 0.21779397 0.22063816]\n",
      " [0.17777005 0.13775055 0.17209499 0.28330189]]\n",
      "[[0.24101552 0.17456573 0.20991287 0.20537138]\n",
      " [0.17926875 0.23444425 0.1645685  0.13086107]\n",
      " [0.1947396  0.13834321 0.21415901 0.18742812]\n",
      " [0.18395506 0.16836089 0.2004053  0.20390473]]\n"
     ]
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "\n",
    "result = nodes_landmarks_similarity(graph215_to_80, ['a stop sign', 'a manhole cover', 'a disabled Parking spot', 'a red building'])\n",
    "print(result[[204, 138, 184, 170]]) \n",
    "      \n",
    "print(result[[111, 109, 78, 80]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23782617 0.16758601 0.17363518 0.1718775 ]\n",
      " [0.18607658 0.21070144 0.175823   0.15714756]\n",
      " [0.19446927 0.15427817 0.2063664  0.21113059]\n",
      " [0.18234348 0.15716581 0.18115307 0.23745567]]\n",
      "[[0.21738133 0.19014567 0.17288697 0.19671348]\n",
      " [0.19572923 0.19102642 0.17323929 0.16324607]\n",
      " [0.17854953 0.16072389 0.19860899 0.18983901]\n",
      " [0.18971071 0.15699568 0.1624265  0.18021673]]\n"
     ]
    }
   ],
   "source": [
    "print(result[[204, 138, 184, 170]]) \n",
    "      \n",
    "print(result[[111, 109, 78, 80]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n"
     ]
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "\n",
    "result2 = nodes_landmarks_similarity(graph215_to_80, ['a stop sign', 'a manhole cover', 'a disabled Parking spot', 'a red building'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20045486 0.20780437 0.19079348 0.16749252]\n",
      " [0.20108801 0.24157122 0.16400416 0.13532601]\n",
      " [0.17445144 0.15703727 0.21779397 0.22063816]\n",
      " [0.17777005 0.13775055 0.17209499 0.28330189]]\n",
      "[[0.24529368 0.18213829 0.21401833 0.2034739 ]\n",
      " [0.18173014 0.22842434 0.17214432 0.13466151]\n",
      " [0.19862068 0.14767142 0.21750203 0.1828877 ]\n",
      " [0.18641327 0.16268645 0.19881625 0.1974327 ]]\n"
     ]
    }
   ],
   "source": [
    "print(result2[[204, 138, 184, 170]]) \n",
    "      \n",
    "print(result2[[111, 109, 78, 80]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n"
     ]
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "\n",
    "result = nodes_landmarks_similarity(graph6, ['a manhole cover', 'a stop sign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24521694, 0.22287041])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_graph = NavigationGraph(\"graphs/graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'l2_dist_threshold': 81, 'cosine_sim_threshold': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 14999, L2 Dist: 82.5000, Cos Sim: 0.7861: 100%|██████████| 15000/15000 [21:56<00:00, 11.40it/s]\n",
      "Iter: 14999, L2 Dist: 92.6250, Cos Sim: 0.7520: 100%|██████████| 15000/15000 [22:03<00:00, 11.33it/s]\n",
      "100%|██████████| 1/1 [44:11<00:00, 2651.29s/it]\n"
     ]
    }
   ],
   "source": [
    "graph7 = fgsm_graph_img_txt(graph, [77], ['a manhole cover'], args={\"learning_rate\":0.1, \"l2_dist_threshold\": 81, \"cosine_sim_threshold\":.85})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n"
     ]
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "\n",
    "result = nodes_landmarks_similarity(graph7, ['a manhole cover', 'a stop sign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22188571, 0.29796207])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Recitification\n"
     ]
    }
   ],
   "source": [
    "from lm_nav.optimal_route import nodes_landmarks_similarity\n",
    "\n",
    "result = nodes_landmarks_similarity(graph, ['a manhole cover', 'a stop sign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18534227, 0.26567292])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(graph6, 'graph6_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(graph7, 'graph7_images')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot_vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
